{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install nltk\n",
    "!pip install requests\n",
    "!pip install python-dotenv\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "# Install required libraries\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "# Load the image from file\n",
    "image_path = 'test4.jpeg'  # Replace with your image file path\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Text:\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"\\nTokenized Sentences:\")\n",
    "    print(sentences)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    print(\"\\nTokenized Words:\")\n",
    "    print(words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    print(\"\\nFiltered Words (Stopwords Removed):\")\n",
    "    print(filtered_words)\n",
    "\n",
    "    # Calculate word frequency\n",
    "    freq_dist = FreqDist(filtered_words)\n",
    "    print(\"\\nWord Frequencies:\")\n",
    "    for word, freq in freq_dist.most_common(10):\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"filtered_words\": filtered_words,\n",
    "        \"freq_dist\": freq_dist\n",
    "    }\n",
    "\n",
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def summarize_with_gemini(api_key, extracted_text, custom_prompt):\n",
    "    \"\"\"\n",
    "    Summarize text using Google's Gemini API\n",
    "    \"\"\"\n",
    "    # Define the API endpoint - using the correct Gemini API URL\n",
    "    endpoint = f\"https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent?key={api_key}\"\n",
    "    \n",
    "    # Construct the payload according to Gemini API specifications\n",
    "    payload = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": f\"Instructions: {custom_prompt}\\n\\nText to analyze: {extracted_text}\"\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"maxOutputTokens\": 300,\n",
    "            \"topP\": 0.8,\n",
    "            \"topK\": 40\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the POST request\n",
    "        response = requests.post(endpoint, headers=headers, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract the generated text from the response\n",
    "            if ('candidates' in response_data and \n",
    "                len(response_data['candidates']) > 0 and \n",
    "                'content' in response_data['candidates'][0] and \n",
    "                'parts' in response_data['candidates'][0]['content'] and \n",
    "                len(response_data['candidates'][0]['content']['parts']) > 0 and \n",
    "                'text' in response_data['candidates'][0]['content']['parts'][0]):\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"summary\": response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Unexpected response structure\"\n",
    "                }\n",
    "        else:\n",
    "            error_message = response.json().get('error', {}).get('message', 'Unknown error occurred')\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"API request failed with status {response.status_code}: {error_message}\"\n",
    "            }\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Request failed: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your API key\n",
    "    api_key = os.getenv('GEMINI_API_KEY') # Replace with actual API key\n",
    "    \n",
    "    # Example text and prompt\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    custom_prompt = \"explain all the dishes?\"\n",
    "    \n",
    "    # Call the function\n",
    "    result = summarize_with_gemini(api_key, extracted_text, custom_prompt)\n",
    "    \n",
    "    # Handle the result\n",
    "    if result.get(\"success\", False):\n",
    "        print(\"Summary:\", result[\"summary\"])\n",
    "    else:\n",
    "        print(\"Error:\", result.get(\"error\", \"An unknown error occurred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def clean_and_normalize_text(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning and normalization using regex and NLTK\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split text into lines and process each menu item\n",
    "    lines = text.split('\\n')\n",
    "    food_items = []\n",
    "    prices = []\n",
    "    \n",
    "    # Flag to identify menu section\n",
    "    in_menu_section = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Start capturing items after the header\n",
    "        if 'item Qty. Rate Total' in line:\n",
    "            in_menu_section = True\n",
    "            continue\n",
    "            \n",
    "        # Stop capturing when we hit totals section\n",
    "        if 'Total Qty:' in line or 'Sub Total:' in line:\n",
    "            in_menu_section = False\n",
    "            continue\n",
    "        \n",
    "        if in_menu_section:\n",
    "            # Try to extract menu items\n",
    "            # Pattern: [Item name] [quantity] [rate] [total]\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:  # Need at least item name, qty, rate, total\n",
    "                try:\n",
    "                    # Last number is the total price\n",
    "                    total = float(parts[-1])\n",
    "                    # Remove the last 3 numbers (qty, rate, total) to get item name\n",
    "                    item_name = ' '.join(parts[:-3])\n",
    "                    # Clean up any special characters\n",
    "                    item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                    \n",
    "                    if item_name and len(item_name) > 2:\n",
    "                        food_items.append(item_name.title())\n",
    "                        prices.append(total)\n",
    "                except ValueError:\n",
    "                    # Handle cases like \"HYDERABADI MURG) BIRYANI\"\n",
    "                    if 'BIRYANI' in line:\n",
    "                        # Special handling for biryani line\n",
    "                        biryani_parts = [p for p in parts if p.replace('.', '').isdigit()]\n",
    "                        if biryani_parts:\n",
    "                            total = float(biryani_parts[-1])\n",
    "                            item_name = ' '.join([p for p in parts if not p.replace('.', '').isdigit()])\n",
    "                            item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                            food_items.append(item_name.title())\n",
    "                            prices.append(total)\n",
    "    \n",
    "    return {\n",
    "        'food_items': food_items,\n",
    "        'prices': prices\n",
    "    }\n",
    "\n",
    "def generate_summary(processed_text):\n",
    "    \"\"\"\n",
    "    Generate a descriptive summary of the food items and bill\n",
    "    \"\"\"\n",
    "    items = processed_text['food_items']\n",
    "    prices = processed_text['prices']\n",
    "    \n",
    "    if not items:\n",
    "        return \"No food items found in the bill.\"\n",
    "    \n",
    "    summary = \"The bill includes \"\n",
    "    if len(items) == 1:\n",
    "        summary += f\"{items[0]}\"\n",
    "    elif len(items) == 2:\n",
    "        summary += f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        summary += \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "    \n",
    "    if prices:\n",
    "        total = sum(prices)\n",
    "        summary += f\". The total amount is Rs. {total:.2f}\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_food_entities_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Use spaCy NER to identify food items and related entities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom food entity patterns\n",
    "        food_patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "        ]\n",
    "        \n",
    "        # Add entity ruler to the pipeline\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        ruler.add_patterns(food_patterns)\n",
    "        \n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Return the identified food entities\n",
    "        food_entities = [ent.text for ent in doc.ents if ent.label_ == \"FOOD\"]\n",
    "        \n",
    "        return food_entities\n",
    "    \n",
    "    except OSError:\n",
    "        print(\"Error: The spaCy English model is not installed.\")\n",
    "        print(\"Please run: !python -m spacy download en_core_web_sm\")\n",
    "        return []\n",
    "\n",
    "# Process the bill text to extract food items\n",
    "food_items = extract_food_entities_with_spacy(extracted_text)\n",
    "print(\"Food items identified using spaCy NER:\")\n",
    "for item in food_items:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def apply_rule_based_classification(text):\n",
    "    \"\"\"\n",
    "    Apply rule-based token classification with regex patterns to identify domain-specific entities\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Dictionary to store all recognized entities\n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # Define patterns for different entity types\n",
    "    patterns = {\n",
    "        'FOOD_ITEM': [\n",
    "            r'(?:Tandoori|Lasooni|HYDERABADI|BIRYANI|Dal|Tadka|Roti|chicken|spicy)\\s*[\\w\\s]*',  # Food items\n",
    "            r'[A-Z][a-z]+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*'  # Capitalized multi-word items\n",
    "        ],\n",
    "        'PRICE': [\n",
    "            r'(?:Rs\\.?|₹)?\\s*\\d+(?:[,.]\\d{1,2})?',  # Indian currency format\n",
    "            r'\\d+\\.\\d{2}'  # Decimal price format\n",
    "        ],\n",
    "        'QUANTITY': [\n",
    "            r'\\b\\d+\\b(?!\\.\\d+)',  # Simple numbers like 1, 2, 3\n",
    "            r'\\d+\\s*x',  # Format: 2x\n",
    "            r'x\\s*\\d+',  # Format: x2\n",
    "            r'\\d+\\s+(?:pcs|items?|pieces)'  # Format: 2 items, 3 pieces\n",
    "        ],\n",
    "        'DATE': [\n",
    "            r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "            r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}'  # 20 May 18\n",
    "        ],\n",
    "        'TIME': [\n",
    "            r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?'  # 12:55, 12:55 PM\n",
    "        ],\n",
    "        'ORDER_NUMBER': [\n",
    "            r'(?:Order|Invoice|Bill)(?:\\s+#|:|\\s+Number:?)\\s*[A-Za-z0-9-]+',  # Invoice Number: IN001001259\n",
    "            r'#\\s*[A-Za-z0-9-]+'  # #12345\n",
    "        ],\n",
    "        'CONTACT': [\n",
    "            r'(?:\\+\\d{1,3}\\s*)?(?:\\(\\d{3,4}\\)\\s*|\\d{3,4}[-\\s])\\d{3,4}[-\\s]\\d{4}',  # Phone number formats\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'  # Email\n",
    "        ],\n",
    "        'ADDRESS': [\n",
    "            r'\\d+/\\d+\\s+\\w+[-\\s]\\s*\\d+',  # 11/2 Sector- 37\n",
    "            r'[A-Z][a-z]+[-\\s]\\s*\\d+'  # Faridabad- 121003\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Apply each pattern and extract matches\n",
    "    for entity_type, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                # Get the matched text and its position\n",
    "                start, end = match.span()\n",
    "                matched_text = match.group()\n",
    "                \n",
    "                # Some post-processing and validation\n",
    "                if entity_type == 'FOOD_ITEM' and len(matched_text) < 3:\n",
    "                    continue  # Skip very short food items\n",
    "                \n",
    "                if entity_type == 'PRICE' and not any(c.isdigit() for c in matched_text):\n",
    "                    continue  # Ensure prices contain digits\n",
    "                \n",
    "                # Store the entity with its position\n",
    "                entities[entity_type].append({\n",
    "                    'text': matched_text,\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def enhance_ner_with_rules(text):\n",
    "    \"\"\"\n",
    "    Combine spaCy NER with rule-based classification to get the best of both approaches\n",
    "    \"\"\"\n",
    "    # Get standard NER results\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom entity patterns\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        \n",
    "        # Define patterns for food items and other domain-specific entities\n",
    "        patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "            {\"label\": \"QUANTITY\", \"pattern\": [{\"IS_DIGIT\": True}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"ddd.dd\"}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"d,ddd.dd\"}]},\n",
    "        ]\n",
    "        \n",
    "        ruler.add_patterns(patterns)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        spacy_entities = {\n",
    "            \"FOOD\": [],\n",
    "            \"CARDINAL\": [],\n",
    "            \"MONEY\": [],\n",
    "            \"DATE\": [],\n",
    "            \"TIME\": [],\n",
    "            \"ORG\": [],\n",
    "            \"PERSON\": [],\n",
    "        }\n",
    "        \n",
    "        # Extract entities recognized by spaCy\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in spacy_entities:\n",
    "                spacy_entities[ent.label_].append({\n",
    "                    'text': ent.text,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in spaCy processing: {e}\")\n",
    "        spacy_entities = {}\n",
    "    \n",
    "    # Get rule-based entities\n",
    "    rule_entities = apply_rule_based_classification(text)\n",
    "    \n",
    "    # Merge the results with priority to rule-based entities\n",
    "    combined_entities = {}\n",
    "    \n",
    "    # First add rule-based entities\n",
    "    for entity_type, entities in rule_entities.items():\n",
    "        combined_entities[entity_type] = entities\n",
    "    \n",
    "    # Then add spaCy entities that don't overlap with rule-based ones\n",
    "    for entity_type, entities in spacy_entities.items():\n",
    "        if entity_type not in combined_entities:\n",
    "            combined_entities[entity_type] = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Check if this entity overlaps with any existing entity\n",
    "            overlaps = False\n",
    "            for existing_type in combined_entities:\n",
    "                for existing_entity in combined_entities[existing_type]:\n",
    "                    # Check for overlap in character spans\n",
    "                    if (entity['start'] < existing_entity['end'] and \n",
    "                        entity['end'] > existing_entity['start']):\n",
    "                        overlaps = True\n",
    "                        break\n",
    "                if overlaps:\n",
    "                    break\n",
    "            \n",
    "            # Add if no overlap\n",
    "            if not overlaps:\n",
    "                combined_entities[entity_type].append(entity)\n",
    "    \n",
    "    return combined_entities\n",
    "\n",
    "print(\"\\nApplying rule-based token classification to receipt text:\")\n",
    "rule_based_entities = apply_rule_based_classification(extracted_text)\n",
    "for entity_type, entities in rule_based_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")\n",
    "\n",
    "print(\"\\nCombining spaCy NER with rule-based classification:\")\n",
    "combined_entities = enhance_ner_with_rules(extracted_text)\n",
    "for entity_type, entities in combined_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
