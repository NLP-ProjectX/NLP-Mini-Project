{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (0.3.13)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pytesseract) (11.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (3.8.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 660.6 kB/s eta 0:00:20\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:13\n",
      "     --------------------------------------- 0.1/12.8 MB 939.4 kB/s eta 0:00:14\n",
      "      --------------------------------------- 0.2/12.8 MB 1.5 MB/s eta 0:00:09\n",
      "     - -------------------------------------- 0.4/12.8 MB 2.0 MB/s eta 0:00:07\n",
      "     -- ------------------------------------- 0.6/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.9/12.8 MB 2.9 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.1/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.0/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 2.2/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 2.4/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.6/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.9/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 3.1/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 4.0 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 3.9/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 4.1 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.4/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.7/12.8 MB 4.2 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 4.9/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 4.3 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.9/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 6.2/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.4/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.7/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 4.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.4/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.3/12.8 MB 4.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.5/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.6/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 8.8/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.3/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.8/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 4.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.3/12.8 MB 4.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 10.9/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.1/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.7/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 4.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.7 MB/s eta 0:00:00\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\nanda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install nltk\n",
    "!pip install requests\n",
    "!pip install python-dotenv\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "# Install required libraries\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "# Load the image from file\n",
    "image_path = 'test4.jpeg'  # Replace with your image file path\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Text:\n",
      "Liquor Street\n",
      "(ODVJH Private Limited)\n",
      "11/2 Sector- 37,\n",
      "Faridabad- 121003.\n",
      "Ph. No. : 0129-4360377, 9311111116\n",
      "GSTIN : O6@AACCO6344G12J\n",
      "\n",
      "Invoice Number: IN001001259\n",
      "Invoice Date: 20-May-18 22:55\n",
      "\n",
      "item Qty. Rate Total\n",
      "Tandoori chicken 4 295.00 309.75\n",
      "Lasooni Dal Tadka 4 275.00 288.75\n",
      "HYDERABADI MURG)\n",
      "BIRYANI 1 375.00 393.75\n",
      "Tandoori Roti all food\n",
      "less spicy 2 30.00 63.00\n",
      "Tandoori Roti 1 30.00 31.50\n",
      "Total Qty: | 6\n",
      "Sub Total: 1,035.00\n",
      "CGST@z2.5 25.89\n",
      "SGST@2.5 25.89\n",
      "Total: 1,139.00\n",
      "\n",
      "Thanks For Visit...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Text:\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Sentences:\n",
      "['Liquor Street\\n(ODVJH Private Limited)\\n11/2 Sector- 37,\\nFaridabad- 121003.', 'Ph.', 'No.', ': 0129-4360377, 9311111116\\nGSTIN : O6@AACCO6344G12J\\n\\nInvoice Number: IN001001259\\nInvoice Date: 20-May-18 22:55\\n\\nitem Qty.', 'Rate Total\\nTandoori chicken 4 295.00 309.75\\nLasooni Dal Tadka 4 275.00 288.75\\nHYDERABADI MURG)\\nBIRYANI 1 375.00 393.75\\nTandoori Roti all food\\nless spicy 2 30.00 63.00\\nTandoori Roti 1 30.00 31.50\\nTotal Qty: | 6\\nSub Total: 1,035.00\\nCGST@z2.5 25.89\\nSGST@2.5 25.89\\nTotal: 1,139.00\\n\\nThanks For Visit...']\n",
      "\n",
      "Tokenized Words:\n",
      "['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', 'No', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'all', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'For', 'Visit', '...']\n",
      "\n",
      "Filtered Words (Stopwords Removed):\n",
      "['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'Visit', '...']\n",
      "\n",
      "Word Frequencies:\n",
      ":: 7\n",
      ".: 4\n",
      "Total: 4\n",
      "@: 3\n",
      "Tandoori: 3\n",
      "): 2\n",
      ",: 2\n",
      "Invoice: 2\n",
      "Qty: 2\n",
      "4: 2\n",
      "{'sentences': ['Liquor Street\\n(ODVJH Private Limited)\\n11/2 Sector- 37,\\nFaridabad- 121003.', 'Ph.', 'No.', ': 0129-4360377, 9311111116\\nGSTIN : O6@AACCO6344G12J\\n\\nInvoice Number: IN001001259\\nInvoice Date: 20-May-18 22:55\\n\\nitem Qty.', 'Rate Total\\nTandoori chicken 4 295.00 309.75\\nLasooni Dal Tadka 4 275.00 288.75\\nHYDERABADI MURG)\\nBIRYANI 1 375.00 393.75\\nTandoori Roti all food\\nless spicy 2 30.00 63.00\\nTandoori Roti 1 30.00 31.50\\nTotal Qty: | 6\\nSub Total: 1,035.00\\nCGST@z2.5 25.89\\nSGST@2.5 25.89\\nTotal: 1,139.00\\n\\nThanks For Visit...'], 'filtered_words': ['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'Visit', '...'], 'freq_dist': FreqDist({':': 7, '.': 4, 'Total': 4, '@': 3, 'Tandoori': 3, ')': 2, ',': 2, 'Invoice': 2, 'Qty': 2, '4': 2, ...})}\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"\\nTokenized Sentences:\")\n",
    "    print(sentences)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    print(\"\\nTokenized Words:\")\n",
    "    print(words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    print(\"\\nFiltered Words (Stopwords Removed):\")\n",
    "    print(filtered_words)\n",
    "\n",
    "    # Calculate word frequency\n",
    "    freq_dist = FreqDist(filtered_words)\n",
    "    print(\"\\nWord Frequencies:\")\n",
    "    for word, freq in freq_dist.most_common(10):\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"filtered_words\": filtered_words,\n",
    "        \"freq_dist\": freq_dist\n",
    "    }\n",
    "\n",
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenized Sentences:\n",
      "['Liquor Street\\n(ODVJH Private Limited)\\n11/2 Sector- 37,\\nFaridabad- 121003.', 'Ph.', 'No.', ': 0129-4360377, 9311111116\\nGSTIN : O6@AACCO6344G12J\\n\\nInvoice Number: IN001001259\\nInvoice Date: 20-May-18 22:55\\n\\nitem Qty.', 'Rate Total\\nTandoori chicken 4 295.00 309.75\\nLasooni Dal Tadka 4 275.00 288.75\\nHYDERABADI MURG)\\nBIRYANI 1 375.00 393.75\\nTandoori Roti all food\\nless spicy 2 30.00 63.00\\nTandoori Roti 1 30.00 31.50\\nTotal Qty: | 6\\nSub Total: 1,035.00\\nCGST@z2.5 25.89\\nSGST@2.5 25.89\\nTotal: 1,139.00\\n\\nThanks For Visit...']\n",
      "\n",
      "Tokenized Words:\n",
      "['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', 'No', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'all', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'For', 'Visit', '...']\n",
      "\n",
      "Filtered Words (Stopwords Removed):\n",
      "['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'Visit', '...']\n",
      "\n",
      "Word Frequencies:\n",
      ":: 7\n",
      ".: 4\n",
      "Total: 4\n",
      "@: 3\n",
      "Tandoori: 3\n",
      "): 2\n",
      ",: 2\n",
      "Invoice: 2\n",
      "Qty: 2\n",
      "4: 2\n",
      "{'sentences': ['Liquor Street\\n(ODVJH Private Limited)\\n11/2 Sector- 37,\\nFaridabad- 121003.', 'Ph.', 'No.', ': 0129-4360377, 9311111116\\nGSTIN : O6@AACCO6344G12J\\n\\nInvoice Number: IN001001259\\nInvoice Date: 20-May-18 22:55\\n\\nitem Qty.', 'Rate Total\\nTandoori chicken 4 295.00 309.75\\nLasooni Dal Tadka 4 275.00 288.75\\nHYDERABADI MURG)\\nBIRYANI 1 375.00 393.75\\nTandoori Roti all food\\nless spicy 2 30.00 63.00\\nTandoori Roti 1 30.00 31.50\\nTotal Qty: | 6\\nSub Total: 1,035.00\\nCGST@z2.5 25.89\\nSGST@2.5 25.89\\nTotal: 1,139.00\\n\\nThanks For Visit...'], 'filtered_words': ['Liquor', 'Street', '(', 'ODVJH', 'Private', 'Limited', ')', '11/2', 'Sector-', '37', ',', 'Faridabad-', '121003', '.', 'Ph', '.', '.', ':', '0129-4360377', ',', '9311111116', 'GSTIN', ':', 'O6', '@', 'AACCO6344G12J', 'Invoice', 'Number', ':', 'IN001001259', 'Invoice', 'Date', ':', '20-May-18', '22:55', 'item', 'Qty', '.', 'Rate', 'Total', 'Tandoori', 'chicken', '4', '295.00', '309.75', 'Lasooni', 'Dal', 'Tadka', '4', '275.00', '288.75', 'HYDERABADI', 'MURG', ')', 'BIRYANI', '1', '375.00', '393.75', 'Tandoori', 'Roti', 'food', 'less', 'spicy', '2', '30.00', '63.00', 'Tandoori', 'Roti', '1', '30.00', '31.50', 'Total', 'Qty', ':', '|', '6', 'Sub', 'Total', ':', '1,035.00', 'CGST', '@', 'z2.5', '25.89', 'SGST', '@', '2.5', '25.89', 'Total', ':', '1,139.00', 'Thanks', 'Visit', '...'], 'freq_dist': FreqDist({':': 7, '.': 4, 'Total': 4, '@': 3, 'Tandoori': 3, ')': 2, ',': 2, 'Invoice': 2, 'Qty': 2, '4': 2, ...})}\n"
     ]
    }
   ],
   "source": [
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: API request failed with status 400: API key not valid. Please pass a valid API key.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def summarize_with_gemini(api_key, extracted_text, custom_prompt):\n",
    "    \"\"\"\n",
    "    Summarize text using Google's Gemini API\n",
    "    \"\"\"\n",
    "    # Define the API endpoint - using the correct Gemini API URL\n",
    "    endpoint = f\"https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent?key={api_key}\"\n",
    "    \n",
    "    # Construct the payload according to Gemini API specifications\n",
    "    payload = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": f\"Instructions: {custom_prompt}\\n\\nText to analyze: {extracted_text}\"\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"maxOutputTokens\": 300,\n",
    "            \"topP\": 0.8,\n",
    "            \"topK\": 40\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the POST request\n",
    "        response = requests.post(endpoint, headers=headers, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract the generated text from the response\n",
    "            if ('candidates' in response_data and \n",
    "                len(response_data['candidates']) > 0 and \n",
    "                'content' in response_data['candidates'][0] and \n",
    "                'parts' in response_data['candidates'][0]['content'] and \n",
    "                len(response_data['candidates'][0]['content']['parts']) > 0 and \n",
    "                'text' in response_data['candidates'][0]['content']['parts'][0]):\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"summary\": response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Unexpected response structure\"\n",
    "                }\n",
    "        else:\n",
    "            error_message = response.json().get('error', {}).get('message', 'Unknown error occurred')\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"API request failed with status {response.status_code}: {error_message}\"\n",
    "            }\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Request failed: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your API key\n",
    "    api_key = os.getenv('GEMINI_API_KEY') # Replace with actual API key\n",
    "    \n",
    "    # Example text and prompt\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    custom_prompt = \"explain all the dishes?\"\n",
    "    \n",
    "    # Call the function\n",
    "    result = summarize_with_gemini(api_key, extracted_text, custom_prompt)\n",
    "    \n",
    "    # Handle the result\n",
    "    if result.get(\"success\", False):\n",
    "        print(\"Summary:\", result[\"summary\"])\n",
    "    else:\n",
    "        print(\"Error:\", result.get(\"error\", \"An unknown error occurred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food items identified using spaCy NER:\n",
      "- Tandoori\n",
      "- chicken\n",
      "- Lasooni\n",
      "- Dal\n",
      "- Tadka\n",
      "- HYDERABADI\n",
      "- MURG\n",
      "- BIRYANI\n",
      "- Tandoori\n",
      "- Roti\n",
      "- spicy\n",
      "- Tandoori\n",
      "- Roti\n"
     ]
    }
   ],
   "source": [
    "def clean_and_normalize_text(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning and normalization using regex and NLTK\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split text into lines and process each menu item\n",
    "    lines = text.split('\\n')\n",
    "    food_items = []\n",
    "    prices = []\n",
    "    \n",
    "    # Flag to identify menu section\n",
    "    in_menu_section = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Start capturing items after the header\n",
    "        if 'item Qty. Rate Total' in line:\n",
    "            in_menu_section = True\n",
    "            continue\n",
    "            \n",
    "        # Stop capturing when we hit totals section\n",
    "        if 'Total Qty:' in line or 'Sub Total:' in line:\n",
    "            in_menu_section = False\n",
    "            continue\n",
    "        \n",
    "        if in_menu_section:\n",
    "            # Try to extract menu items\n",
    "            # Pattern: [Item name] [quantity] [rate] [total]\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:  # Need at least item name, qty, rate, total\n",
    "                try:\n",
    "                    # Last number is the total price\n",
    "                    total = float(parts[-1])\n",
    "                    # Remove the last 3 numbers (qty, rate, total) to get item name\n",
    "                    item_name = ' '.join(parts[:-3])\n",
    "                    # Clean up any special characters\n",
    "                    item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                    \n",
    "                    if item_name and len(item_name) > 2:\n",
    "                        food_items.append(item_name.title())\n",
    "                        prices.append(total)\n",
    "                except ValueError:\n",
    "                    # Handle cases like \"HYDERABADI MURG) BIRYANI\"\n",
    "                    if 'BIRYANI' in line:\n",
    "                        # Special handling for biryani line\n",
    "                        biryani_parts = [p for p in parts if p.replace('.', '').isdigit()]\n",
    "                        if biryani_parts:\n",
    "                            total = float(biryani_parts[-1])\n",
    "                            item_name = ' '.join([p for p in parts if not p.replace('.', '').isdigit()])\n",
    "                            item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                            food_items.append(item_name.title())\n",
    "                            prices.append(total)\n",
    "    \n",
    "    return {\n",
    "        'food_items': food_items,\n",
    "        'prices': prices\n",
    "    }\n",
    "\n",
    "def generate_summary(processed_text):\n",
    "    \"\"\"\n",
    "    Generate a descriptive summary of the food items and bill\n",
    "    \"\"\"\n",
    "    items = processed_text['food_items']\n",
    "    prices = processed_text['prices']\n",
    "    \n",
    "    if not items:\n",
    "        return \"No food items found in the bill.\"\n",
    "    \n",
    "    summary = \"The bill includes \"\n",
    "    if len(items) == 1:\n",
    "        summary += f\"{items[0]}\"\n",
    "    elif len(items) == 2:\n",
    "        summary += f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        summary += \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "    \n",
    "    if prices:\n",
    "        total = sum(prices)\n",
    "        summary += f\". The total amount is Rs. {total:.2f}\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_food_entities_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Use spaCy NER to identify food items and related entities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom food entity patterns\n",
    "        food_patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "        ]\n",
    "        \n",
    "        # Add entity ruler to the pipeline\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        ruler.add_patterns(food_patterns)\n",
    "        \n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Return the identified food entities\n",
    "        food_entities = [ent.text for ent in doc.ents if ent.label_ == \"FOOD\"]\n",
    "        \n",
    "        return food_entities\n",
    "    \n",
    "    except OSError:\n",
    "        print(\"Error: The spaCy English model is not installed.\")\n",
    "        print(\"Please run: !python -m spacy download en_core_web_sm\")\n",
    "        return []\n",
    "\n",
    "# Process the bill text to extract food items\n",
    "food_items = extract_food_entities_with_spacy(extracted_text)\n",
    "print(\"Food items identified using spaCy NER:\")\n",
    "for item in food_items:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying rule-based token classification to receipt text:\n",
      "\n",
      "FOOD_ITEM:\n",
      "  • Tandoori chicken 4 295\n",
      "  • Lasooni Dal Tadka 4 275\n",
      "  • HYDERABADI MURG\n",
      "  • BIRYANI 1 375\n",
      "  • Tandoori Roti all food\n",
      "less spicy 2 30\n",
      "  • Tandoori Roti 1 30\n",
      "  • Liquor Street\n",
      "  • Private Limited\n",
      "  • Invoice Number\n",
      "  • Invoice Date\n",
      "  • Rate Total\n",
      "Tandoori\n",
      "  • Lasooni Dal Tadka\n",
      "  • Tandoori Roti\n",
      "  • Tandoori Roti\n",
      "  • Total Qty\n",
      "  • Sub Total\n",
      "  • Thanks For Visit\n",
      "\n",
      "PRICE:\n",
      "  • \n",
      "11\n",
      "  • 2\n",
      "  •  37\n",
      "  •  121003\n",
      "  •  0129\n",
      "  • 4360377\n",
      "  •  9311111116\n",
      "  • 6\n",
      "  • 6344\n",
      "  • 12\n",
      "  • 001001259\n",
      "  •  20\n",
      "  • 18\n",
      "  •  22\n",
      "  • 55\n",
      "  •  4\n",
      "  •  295.00\n",
      "  •  309.75\n",
      "  •  4\n",
      "  •  275.00\n",
      "  •  288.75\n",
      "  •  1\n",
      "  •  375.00\n",
      "  •  393.75\n",
      "  •  2\n",
      "  •  30.00\n",
      "  •  63.00\n",
      "  •  1\n",
      "  •  30.00\n",
      "  •  31.50\n",
      "  •  6\n",
      "  •  1,03\n",
      "  • 5.00\n",
      "  • 2.5\n",
      "  •  25.89\n",
      "  • 2.5\n",
      "  •  25.89\n",
      "  •  1,13\n",
      "  • 9.00\n",
      "  • 295.00\n",
      "  • 309.75\n",
      "  • 275.00\n",
      "  • 288.75\n",
      "  • 375.00\n",
      "  • 393.75\n",
      "  • 30.00\n",
      "  • 63.00\n",
      "  • 30.00\n",
      "  • 31.50\n",
      "  • 035.00\n",
      "  • 25.89\n",
      "  • 25.89\n",
      "  • 139.00\n",
      "\n",
      "QUANTITY:\n",
      "  • 11\n",
      "  • 2\n",
      "  • 37\n",
      "  • 121003\n",
      "  • 0129\n",
      "  • 4360377\n",
      "  • 9311111116\n",
      "  • 20\n",
      "  • 18\n",
      "  • 22\n",
      "  • 55\n",
      "  • 4\n",
      "  • 00\n",
      "  • 75\n",
      "  • 4\n",
      "  • 00\n",
      "  • 75\n",
      "  • 1\n",
      "  • 00\n",
      "  • 75\n",
      "  • 2\n",
      "  • 00\n",
      "  • 00\n",
      "  • 1\n",
      "  • 00\n",
      "  • 50\n",
      "  • 6\n",
      "  • 1\n",
      "  • 00\n",
      "  • 5\n",
      "  • 89\n",
      "  • 5\n",
      "  • 89\n",
      "  • 1\n",
      "  • 00\n",
      "  • 55\n",
      "\n",
      "item\n",
      "\n",
      "TIME:\n",
      "  • 22:55\n",
      "\n",
      "\n",
      "\n",
      "ORDER_NUMBER:\n",
      "  • Invoice Number: IN001001259\n",
      "\n",
      "ADDRESS:\n",
      "  • 11/2 Sector- 37\n",
      "  • Sector- 37\n",
      "  • Faridabad- 121003\n",
      "  • May-18\n",
      "  • Tadka 4\n",
      "  • Roti 1\n",
      "\n",
      "Combining spaCy NER with rule-based classification:\n",
      "\n",
      "FOOD_ITEM:\n",
      "  • Tandoori chicken 4 295\n",
      "  • Lasooni Dal Tadka 4 275\n",
      "  • HYDERABADI MURG\n",
      "  • BIRYANI 1 375\n",
      "  • Tandoori Roti all food\n",
      "less spicy 2 30\n",
      "  • Tandoori Roti 1 30\n",
      "  • Liquor Street\n",
      "  • Private Limited\n",
      "  • Invoice Number\n",
      "  • Invoice Date\n",
      "  • Rate Total\n",
      "Tandoori\n",
      "  • Lasooni Dal Tadka\n",
      "  • Tandoori Roti\n",
      "  • Tandoori Roti\n",
      "  • Total Qty\n",
      "  • Sub Total\n",
      "  • Thanks For Visit\n",
      "\n",
      "PRICE:\n",
      "  • \n",
      "11\n",
      "  • 2\n",
      "  •  37\n",
      "  •  121003\n",
      "  •  0129\n",
      "  • 4360377\n",
      "  •  9311111116\n",
      "  • 6\n",
      "  • 6344\n",
      "  • 12\n",
      "  • 001001259\n",
      "  •  20\n",
      "  • 18\n",
      "  •  22\n",
      "  • 55\n",
      "  •  4\n",
      "  •  295.00\n",
      "  •  309.75\n",
      "  •  4\n",
      "  •  275.00\n",
      "  •  288.75\n",
      "  •  1\n",
      "  •  375.00\n",
      "  •  393.75\n",
      "  •  2\n",
      "  •  30.00\n",
      "  •  63.00\n",
      "  •  1\n",
      "  •  30.00\n",
      "  •  31.50\n",
      "  •  6\n",
      "  •  1,03\n",
      "  • 5.00\n",
      "  • 2.5\n",
      "  •  25.89\n",
      "  • 2.5\n",
      "  •  25.89\n",
      "  •  1,13\n",
      "  • 9.00\n",
      "  • 295.00\n",
      "  • 309.75\n",
      "  • 275.00\n",
      "  • 288.75\n",
      "  • 375.00\n",
      "  • 393.75\n",
      "  • 30.00\n",
      "  • 63.00\n",
      "  • 30.00\n",
      "  • 31.50\n",
      "  • 035.00\n",
      "  • 25.89\n",
      "  • 25.89\n",
      "  • 139.00\n",
      "\n",
      "QUANTITY:\n",
      "  • 11\n",
      "  • 2\n",
      "  • 37\n",
      "  • 121003\n",
      "  • 0129\n",
      "  • 4360377\n",
      "  • 9311111116\n",
      "  • 20\n",
      "  • 18\n",
      "  • 22\n",
      "  • 55\n",
      "  • 4\n",
      "  • 00\n",
      "  • 75\n",
      "  • 4\n",
      "  • 00\n",
      "  • 75\n",
      "  • 1\n",
      "  • 00\n",
      "  • 75\n",
      "  • 2\n",
      "  • 00\n",
      "  • 00\n",
      "  • 1\n",
      "  • 00\n",
      "  • 50\n",
      "  • 6\n",
      "  • 1\n",
      "  • 00\n",
      "  • 5\n",
      "  • 89\n",
      "  • 5\n",
      "  • 89\n",
      "  • 1\n",
      "  • 00\n",
      "  • 55\n",
      "\n",
      "item\n",
      "\n",
      "TIME:\n",
      "  • 22:55\n",
      "\n",
      "\n",
      "\n",
      "ORDER_NUMBER:\n",
      "  • Invoice Number: IN001001259\n",
      "\n",
      "ADDRESS:\n",
      "  • 11/2 Sector- 37\n",
      "  • Sector- 37\n",
      "  • Faridabad- 121003\n",
      "  • May-18\n",
      "  • Tadka 4\n",
      "  • Roti 1\n",
      "\n",
      "PERSON:\n",
      "  • GSTIN\n"
     ]
    }
   ],
   "source": [
    "def apply_rule_based_classification(text):\n",
    "    \"\"\"\n",
    "    Apply rule-based token classification with regex patterns to identify domain-specific entities\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Dictionary to store all recognized entities\n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # Define patterns for different entity types\n",
    "    patterns = {\n",
    "        'FOOD_ITEM': [\n",
    "            r'(?:Tandoori|Lasooni|HYDERABADI|BIRYANI|Dal|Tadka|Roti|chicken|spicy)\\s*[\\w\\s]*',  # Food items\n",
    "            r'[A-Z][a-z]+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*'  # Capitalized multi-word items\n",
    "        ],\n",
    "        'PRICE': [\n",
    "            r'(?:Rs\\.?|₹)?\\s*\\d+(?:[,.]\\d{1,2})?',  # Indian currency format\n",
    "            r'\\d+\\.\\d{2}'  # Decimal price format\n",
    "        ],\n",
    "        'QUANTITY': [\n",
    "            r'\\b\\d+\\b(?!\\.\\d+)',  # Simple numbers like 1, 2, 3\n",
    "            r'\\d+\\s*x',  # Format: 2x\n",
    "            r'x\\s*\\d+',  # Format: x2\n",
    "            r'\\d+\\s+(?:pcs|items?|pieces)'  # Format: 2 items, 3 pieces\n",
    "        ],\n",
    "        'DATE': [\n",
    "            r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "            r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}'  # 20 May 18\n",
    "        ],\n",
    "        'TIME': [\n",
    "            r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?'  # 12:55, 12:55 PM\n",
    "        ],\n",
    "        'ORDER_NUMBER': [\n",
    "            r'(?:Order|Invoice|Bill)(?:\\s+#|:|\\s+Number:?)\\s*[A-Za-z0-9-]+',  # Invoice Number: IN001001259\n",
    "            r'#\\s*[A-Za-z0-9-]+'  # #12345\n",
    "        ],\n",
    "        'CONTACT': [\n",
    "            r'(?:\\+\\d{1,3}\\s*)?(?:\\(\\d{3,4}\\)\\s*|\\d{3,4}[-\\s])\\d{3,4}[-\\s]\\d{4}',  # Phone number formats\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'  # Email\n",
    "        ],\n",
    "        'ADDRESS': [\n",
    "            r'\\d+/\\d+\\s+\\w+[-\\s]\\s*\\d+',  # 11/2 Sector- 37\n",
    "            r'[A-Z][a-z]+[-\\s]\\s*\\d+'  # Faridabad- 121003\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Apply each pattern and extract matches\n",
    "    for entity_type, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                # Get the matched text and its position\n",
    "                start, end = match.span()\n",
    "                matched_text = match.group()\n",
    "                \n",
    "                # Some post-processing and validation\n",
    "                if entity_type == 'FOOD_ITEM' and len(matched_text) < 3:\n",
    "                    continue  # Skip very short food items\n",
    "                \n",
    "                if entity_type == 'PRICE' and not any(c.isdigit() for c in matched_text):\n",
    "                    continue  # Ensure prices contain digits\n",
    "                \n",
    "                # Store the entity with its position\n",
    "                entities[entity_type].append({\n",
    "                    'text': matched_text,\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def enhance_ner_with_rules(text):\n",
    "    \"\"\"\n",
    "    Combine spaCy NER with rule-based classification to get the best of both approaches\n",
    "    \"\"\"\n",
    "    # Get standard NER results\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom entity patterns\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        \n",
    "        # Define patterns for food items and other domain-specific entities\n",
    "        patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "            {\"label\": \"QUANTITY\", \"pattern\": [{\"IS_DIGIT\": True}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"ddd.dd\"}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"d,ddd.dd\"}]},\n",
    "        ]\n",
    "        \n",
    "        ruler.add_patterns(patterns)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        spacy_entities = {\n",
    "            \"FOOD\": [],\n",
    "            \"CARDINAL\": [],\n",
    "            \"MONEY\": [],\n",
    "            \"DATE\": [],\n",
    "            \"TIME\": [],\n",
    "            \"ORG\": [],\n",
    "            \"PERSON\": [],\n",
    "        }\n",
    "        \n",
    "        # Extract entities recognized by spaCy\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in spacy_entities:\n",
    "                spacy_entities[ent.label_].append({\n",
    "                    'text': ent.text,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in spaCy processing: {e}\")\n",
    "        spacy_entities = {}\n",
    "    \n",
    "    # Get rule-based entities\n",
    "    rule_entities = apply_rule_based_classification(text)\n",
    "    \n",
    "    # Merge the results with priority to rule-based entities\n",
    "    combined_entities = {}\n",
    "    \n",
    "    # First add rule-based entities\n",
    "    for entity_type, entities in rule_entities.items():\n",
    "        combined_entities[entity_type] = entities\n",
    "    \n",
    "    # Then add spaCy entities that don't overlap with rule-based ones\n",
    "    for entity_type, entities in spacy_entities.items():\n",
    "        if entity_type not in combined_entities:\n",
    "            combined_entities[entity_type] = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Check if this entity overlaps with any existing entity\n",
    "            overlaps = False\n",
    "            for existing_type in combined_entities:\n",
    "                for existing_entity in combined_entities[existing_type]:\n",
    "                    # Check for overlap in character spans\n",
    "                    if (entity['start'] < existing_entity['end'] and \n",
    "                        entity['end'] > existing_entity['start']):\n",
    "                        overlaps = True\n",
    "                        break\n",
    "                if overlaps:\n",
    "                    break\n",
    "            \n",
    "            # Add if no overlap\n",
    "            if not overlaps:\n",
    "                combined_entities[entity_type].append(entity)\n",
    "    \n",
    "    return combined_entities\n",
    "\n",
    "print(\"\\nApplying rule-based token classification to receipt text:\")\n",
    "rule_based_entities = apply_rule_based_classification(extracted_text)\n",
    "for entity_type, entities in rule_based_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")\n",
    "\n",
    "print(\"\\nCombining spaCy NER with rule-based classification:\")\n",
    "combined_entities = enhance_ner_with_rules(extracted_text)\n",
    "for entity_type, entities in combined_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (4.50.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (6.5.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.21.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nanda\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ensorflow (c:\\users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries for BERT model implementation\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for BERT implementation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Extracted Text:\n",
      "Liquor Street\n",
      "(ODVJH Private Limited)\n",
      "11/2 Sector- 37,\n",
      "Faridabad- 121003.\n",
      "Ph. No. : 0129-4360377, 9311111116\n",
      "GSTIN : O6@AACCO6344G12J\n",
      "\n",
      "Invoice Number: IN001001259\n",
      "Invoice Date: 20-May-18 22:55\n",
      "\n",
      "item Qty. Rate Total\n",
      "Tandoori chicken 4 295.00 309.75\n",
      "Lasooni Dal Tadka 4 275.00 288.75\n",
      "HYDERABADI MURG)\n",
      "BIRYANI 1 375.00 393.75\n",
      "Tandoori Roti all food\n",
      "less spicy 2 30.00 63.00\n",
      "Tandoori Roti 1 30.00 31.50\n",
      "Total Qty: | 6\n",
      "Sub Total: 1,035.00\n",
      "CGST@z2.5 25.89\n",
      "SGST@2.5 25.89\n",
      "Total: 1,139.00\n",
      "\n",
      "Thanks For Visit.. ...\n",
      "\n",
      "Cleaned Text for BERT:\n",
      "Liquor Street (ODVJH Private Limited) 112 Sector- 37, Faridabad- 121003. Ph. No. : 0129-4360377, 9311111116 GSTIN : O6AACCO6344G12J Invoice Number: IN001001259 Invoice Date: 20-May-18 22:55 item Qty. Rate Total Tandoori chicken 4 295.00 309.75 Lasooni Dal Tadka 4 275.00 288.75 HYDERABADI MURG) BIRYANI 1 375.00 393.75 Tandoori Roti all food less spicy 2 30.00 63.00 Tandoori Roti 1 30.00 31.50 Total Qty:  6 Sub Total: 1,035.00 CGSTz2.5 25.89 SGST2.5 25.89 Total: 1,139.00 Thanks For Visit... ...\n"
     ]
    }
   ],
   "source": [
    "# Integrate cropping and OCR into a single function\n",
    "def process_image_to_text(image_path):\n",
    "    \"\"\"\n",
    "    Process an image: read, crop, and extract text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Extract text using pytesseract\n",
    "        extracted_text = pytesseract.image_to_string(image)\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        \n",
    "        return {\n",
    "            \"original_text\": extracted_text,\n",
    "            \"cleaned_text\": cleaned_text,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for BERT processing\n",
    "    \"\"\"\n",
    "    # Remove special characters and normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?$%&()-]', '', text)  # Keep only alphanumeric and basic punctuation\n",
    "    \n",
    "    # Remove redundant line breaks\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test the function with your image\n",
    "result = process_image_to_text(image_path)\n",
    "if result[\"success\"]:\n",
    "    print(\"Original Extracted Text:\")\n",
    "    print(result[\"original_text\"][:500], \"...\\n\")\n",
    "    print(\"Cleaned Text for BERT:\")\n",
    "    print(result[\"cleaned_text\"][:500], \"...\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nanda\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "✅ Auto-labeling complete! Data saved as auto_labeled_receipt_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1 Training: 100%|██████████| 18/18 [00:04<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 1.0345, Accuracy = 0.6496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 Training: 100%|██████████| 18/18 [00:03<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss = 0.8413, Accuracy = 0.6861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 Training: 100%|██████████| 18/18 [00:02<00:00,  6.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss = 0.7799, Accuracy = 0.6934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 Training: 100%|██████████| 18/18 [00:01<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss = 0.7806, Accuracy = 0.7007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 Training: 100%|██████████| 18/18 [00:01<00:00,  9.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss = 0.6083, Accuracy = 0.7664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 Training: 100%|██████████| 18/18 [00:01<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss = 0.5289, Accuracy = 0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 Training: 100%|██████████| 18/18 [00:02<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss = 0.5177, Accuracy = 0.8175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 Training: 100%|██████████| 18/18 [00:02<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss = 0.3902, Accuracy = 0.8978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 Training: 100%|██████████| 18/18 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss = 0.2566, Accuracy = 0.9343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 Training: 100%|██████████| 18/18 [00:02<00:00,  8.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss = 0.1748, Accuracy = 0.9708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 Training: 100%|██████████| 18/18 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss = 0.1282, Accuracy = 0.9854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 Training: 100%|██████████| 18/18 [00:02<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss = 0.1069, Accuracy = 0.9854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 Training: 100%|██████████| 18/18 [00:03<00:00,  5.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss = 0.0729, Accuracy = 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 Training: 100%|██████████| 18/18 [00:03<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss = 0.0602, Accuracy = 0.9927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 Training: 100%|██████████| 18/18 [00:03<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss = 0.0431, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 Training: 100%|██████████| 18/18 [00:02<00:00,  6.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss = 0.0382, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 Training: 100%|██████████| 18/18 [00:02<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss = 0.0317, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 Training: 100%|██████████| 18/18 [00:01<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss = 0.0352, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 Training: 100%|██████████| 18/18 [00:01<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss = 0.0235, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 Training: 100%|██████████| 18/18 [00:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss = 0.0197, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 Training: 100%|██████████| 18/18 [00:01<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: Loss = 0.0186, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 Training: 100%|██████████| 18/18 [00:01<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: Loss = 0.0158, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 Training: 100%|██████████| 18/18 [00:01<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: Loss = 0.0150, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 Training: 100%|██████████| 18/18 [00:02<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: Loss = 0.0142, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 Training: 100%|██████████| 18/18 [00:02<00:00,  8.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: Loss = 0.0121, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 Training: 100%|██████████| 18/18 [00:02<00:00,  8.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: Loss = 0.0105, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 Training: 100%|██████████| 18/18 [00:02<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27: Loss = 0.0103, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 Training: 100%|██████████| 18/18 [00:02<00:00,  6.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28: Loss = 0.0103, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 Training: 100%|██████████| 18/18 [00:03<00:00,  5.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29: Loss = 0.0090, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 Training: 100%|██████████| 18/18 [00:02<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30: Loss = 0.0075, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31 Training: 100%|██████████| 18/18 [00:02<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31: Loss = 0.0076, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32 Training: 100%|██████████| 18/18 [00:02<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32: Loss = 0.0071, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33 Training: 100%|██████████| 18/18 [00:03<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33: Loss = 0.0074, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34 Training: 100%|██████████| 18/18 [00:02<00:00,  6.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34: Loss = 0.0064, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35 Training: 100%|██████████| 18/18 [00:02<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35: Loss = 0.0057, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36 Training: 100%|██████████| 18/18 [00:03<00:00,  5.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36: Loss = 0.0056, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37 Training: 100%|██████████| 18/18 [00:03<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37: Loss = 0.0053, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38 Training: 100%|██████████| 18/18 [00:02<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: Loss = 0.0051, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39 Training: 100%|██████████| 18/18 [00:03<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39: Loss = 0.0051, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40 Training: 100%|██████████| 18/18 [00:02<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40: Loss = 0.0047, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41 Training: 100%|██████████| 18/18 [00:03<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41: Loss = 0.0046, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42 Training: 100%|██████████| 18/18 [00:02<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42: Loss = 0.0042, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43 Training: 100%|██████████| 18/18 [00:03<00:00,  5.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43: Loss = 0.0038, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44 Training: 100%|██████████| 18/18 [00:02<00:00,  6.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44: Loss = 0.0049, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45 Training: 100%|██████████| 18/18 [00:03<00:00,  5.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45: Loss = 0.0038, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46 Training: 100%|██████████| 18/18 [00:03<00:00,  5.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46: Loss = 0.0035, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47 Training: 100%|██████████| 18/18 [00:02<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47: Loss = 0.0033, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48 Training: 100%|██████████| 18/18 [00:02<00:00,  6.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48: Loss = 0.0036, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49 Training: 100%|██████████| 18/18 [00:02<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: Loss = 0.0031, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50 Training: 100%|██████████| 18/18 [00:02<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50: Loss = 0.0032, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51 Training: 100%|██████████| 18/18 [00:02<00:00,  6.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51: Loss = 0.0032, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52 Training: 100%|██████████| 18/18 [00:03<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52: Loss = 0.0029, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53 Training: 100%|██████████| 18/18 [00:03<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53: Loss = 0.0034, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54 Training: 100%|██████████| 18/18 [00:02<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54: Loss = 0.0026, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55 Training: 100%|██████████| 18/18 [00:03<00:00,  5.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55: Loss = 0.0025, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56 Training: 100%|██████████| 18/18 [00:02<00:00,  6.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56: Loss = 0.0023, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57 Training: 100%|██████████| 18/18 [00:02<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57: Loss = 0.0023, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58 Training: 100%|██████████| 18/18 [00:01<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58: Loss = 0.0022, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59 Training: 100%|██████████| 18/18 [00:01<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59: Loss = 0.0022, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60 Training: 100%|██████████| 18/18 [00:01<00:00,  9.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60: Loss = 0.0024, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61 Training: 100%|██████████| 18/18 [00:02<00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61: Loss = 0.0020, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62 Training: 100%|██████████| 18/18 [00:01<00:00,  9.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62: Loss = 0.0020, Accuracy = 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63 Training:  94%|█████████▍| 17/18 [00:01<00:00,  8.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 142\u001b[0m\n\u001b[0;32m    138\u001b[0m             total_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    140\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_train_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_correct\u001b[38;5;241m/\u001b[39mtotal_samples\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;66;03m# 🔹 Step 8: Evaluate the Model\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_model\u001b[39m(model, test_dataloader):\n",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, epochs)\u001b[0m\n\u001b[0;32m    131\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels)\n\u001b[0;32m    132\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 133\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    136\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\adamw.py:209\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    206\u001b[0m     amsgrad: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    207\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m cast(Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m], group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 209\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     adamw(\n\u001b[0;32m    221\u001b[0m         params_with_grad,\n\u001b[0;32m    222\u001b[0m         grads,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[0;32m    241\u001b[0m     )\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\nanda\\miniconda3\\envs\\tf\\lib\\site-packages\\torch\\optim\\adamw.py:124\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[1;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m has_complex \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m params_with_grad\u001b[38;5;241m.\u001b[39mappend(p)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 🔹 Step 1: OCR Setup\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# 🔹 Step 2: Extract Text from Cropped Receipts\n",
    "def extract_text_from_images(folder_path):\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            text = pytesseract.image_to_string(img)  # OCR extraction\n",
    "            data.append({\"filename\": file, \"text\": text.strip()})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "folder_path = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\NLP-Mini-Project\\Cropped_Receipts\"\n",
    "df = extract_text_from_images(folder_path)\n",
    "\n",
    "# 🔹 Step 3: Automated Labeling Rules\n",
    "def classify_text(text):\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Rule 1: Amount Detection ($XX.XX or XX.XX format)\n",
    "    if re.search(r\"\\$\\d+\\.\\d{2}|\\b\\d+\\.\\d{2}\\b\", text):\n",
    "        return \"total_amount\"\n",
    "\n",
    "    # Rule 2: Date Detection (MM/DD/YYYY or DD/MM/YYYY)\n",
    "    if re.search(r\"\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b\", text):\n",
    "        return \"date\"\n",
    "\n",
    "    # Rule 3: Restaurant Name Detection (Known Chains)\n",
    "    restaurant_keywords = [\"McDonald's\", \"Subway\", \"Starbucks\", \"KFC\", \"Pizza Hut\", \"Domino's\", \"Burger King\"]\n",
    "    if any(kw.lower() in text.lower() for kw in restaurant_keywords):\n",
    "        return \"restaurant_name\"\n",
    "\n",
    "    # Rule 4: Item List Detection (Comma-separated items)\n",
    "    if \",\" in text and len(text.split()) > 2:\n",
    "        return \"items\"\n",
    "    \n",
    "    return \"other\"\n",
    "\n",
    "df[\"category\"] = df[\"text\"].apply(classify_text)\n",
    "\n",
    "# 🔹 Step 4: Save Auto-Labeled Data for BERT Training\n",
    "df = df[df[\"category\"] != \"other\"]  # Remove unclassified\n",
    "df.to_csv(\"auto_labeled_receipt_data.csv\", index=False)\n",
    "\n",
    "print(\"✅ Auto-labeling complete! Data saved as auto_labeled_receipt_data.csv\")\n",
    "\n",
    "# 🔹 Step 5: Load Tokenizer & Prepare Data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "category_mapping = {cat: idx for idx, cat in enumerate(df[\"category\"].unique())}\n",
    "df[\"label\"] = df[\"category\"].map(category_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "class ReceiptDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding=\"max_length\",\n",
    "            truncation=True, return_attention_mask=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = ReceiptDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_len=64)\n",
    "test_dataset = ReceiptDataset(X_test.tolist(), y_test.tolist(), tokenizer, max_len=64)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=8)\n",
    "\n",
    "# 🔹 Step 6: Define BERT Model\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_classes=len(category_mapping)).to(device)\n",
    "\n",
    "# 🔹 Step 7: Train the Model\n",
    "def train_model(model, train_dataloader, epochs=100):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_train_loss/len(train_dataloader):.4f}, Accuracy = {total_correct/total_samples:.4f}\")\n",
    "\n",
    "train_model(model, train_dataloader, epochs=100)\n",
    "\n",
    "# 🔹 Step 8: Evaluate the Model\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Fix: Get only the present labels in predictions\n",
    "    present_labels = sorted(set(all_labels))  \n",
    "    target_names = [k for k, v in category_mapping.items() if v in present_labels]\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, labels=present_labels, target_names=target_names))\n",
    "\n",
    "evaluate_model(model, test_dataloader)\n",
    "\n",
    "# 🔹 Step 9: Predict on New Receipt Text\n",
    "def predict_category(text):\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(text, return_tensors=\"pt\", max_length=64, padding=\"max_length\", truncation=True)\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    category = [k for k, v in category_mapping.items() if v == pred][0]\n",
    "    return category\n",
    "\n",
    "# Example Prediction\n",
    "sample_text = \"$14.89\"\n",
    "print(f\"Predicted Category: {predict_category(sample_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
