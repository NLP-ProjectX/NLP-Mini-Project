{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install pytesseract\n",
    "!pip install nltk\n",
    "!pip install requests\n",
    "!pip install python-dotenv\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "# Install required libraries\n",
    "import requests\n",
    "import json\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:/Program Files/Tesseract-OCR/tesseract.exe'\n",
    "\n",
    "# Load the image from file\n",
    "image_path = 'test4.jpeg'  # Replace with your image file path\n",
    "image = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "extracted_text = pytesseract.image_to_string(image)\n",
    "\n",
    "# Print the extracted text\n",
    "print(\"Extracted Text:\")\n",
    "print(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenize into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    print(\"\\nTokenized Sentences:\")\n",
    "    print(sentences)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    print(\"\\nTokenized Words:\")\n",
    "    print(words)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    print(\"\\nFiltered Words (Stopwords Removed):\")\n",
    "    print(filtered_words)\n",
    "\n",
    "    # Calculate word frequency\n",
    "    freq_dist = FreqDist(filtered_words)\n",
    "    print(\"\\nWord Frequencies:\")\n",
    "    for word, freq in freq_dist.most_common(10):\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "    return {\n",
    "        \"sentences\": sentences,\n",
    "        \"filtered_words\": filtered_words,\n",
    "        \"freq_dist\": freq_dist\n",
    "    }\n",
    "\n",
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Process the extracted text\n",
    "processed_data = preprocess_text(extracted_text)\n",
    "print(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def summarize_with_gemini(api_key, extracted_text, custom_prompt):\n",
    "    \"\"\"\n",
    "    Summarize text using Google's Gemini API\n",
    "    \"\"\"\n",
    "    # Define the API endpoint - using the correct Gemini API URL\n",
    "    endpoint = f\"https://generativelanguage.googleapis.com/v1/models/gemini-pro:generateContent?key={api_key}\"\n",
    "    \n",
    "    # Construct the payload according to Gemini API specifications\n",
    "    payload = {\n",
    "        \"contents\": [{\n",
    "            \"parts\": [{\n",
    "                \"text\": f\"Instructions: {custom_prompt}\\n\\nText to analyze: {extracted_text}\"\n",
    "            }]\n",
    "        }],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"maxOutputTokens\": 300,\n",
    "            \"topP\": 0.8,\n",
    "            \"topK\": 40\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send the POST request\n",
    "        response = requests.post(endpoint, headers=headers, json=payload)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            \n",
    "            # Extract the generated text from the response\n",
    "            if ('candidates' in response_data and \n",
    "                len(response_data['candidates']) > 0 and \n",
    "                'content' in response_data['candidates'][0] and \n",
    "                'parts' in response_data['candidates'][0]['content'] and \n",
    "                len(response_data['candidates'][0]['content']['parts']) > 0 and \n",
    "                'text' in response_data['candidates'][0]['content']['parts'][0]):\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"summary\": response_data['candidates'][0]['content']['parts'][0]['text']\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": False,\n",
    "                    \"error\": \"Unexpected response structure\"\n",
    "                }\n",
    "        else:\n",
    "            error_message = response.json().get('error', {}).get('message', 'Unknown error occurred')\n",
    "            return {\n",
    "                \"success\": False,\n",
    "                \"error\": f\"API request failed with status {response.status_code}: {error_message}\"\n",
    "            }\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": f\"Request failed: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Your API key\n",
    "    api_key = os.getenv('GEMINI_API_KEY') # Replace with actual API key\n",
    "    \n",
    "    # Example text and prompt\n",
    "    extracted_text = pytesseract.image_to_string(image)\n",
    "    custom_prompt = \"explain all the dishes?\"\n",
    "    \n",
    "    # Call the function\n",
    "    result = summarize_with_gemini(api_key, extracted_text, custom_prompt)\n",
    "    \n",
    "    # Handle the result\n",
    "    if result.get(\"success\", False):\n",
    "        print(\"Summary:\", result[\"summary\"])\n",
    "    else:\n",
    "        print(\"Error:\", result.get(\"error\", \"An unknown error occurred\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def clean_and_normalize_text(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning and normalization using regex and NLTK\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Split text into lines and process each menu item\n",
    "    lines = text.split('\\n')\n",
    "    food_items = []\n",
    "    prices = []\n",
    "    \n",
    "    # Flag to identify menu section\n",
    "    in_menu_section = False\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        # Skip empty lines\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Start capturing items after the header\n",
    "        if 'item Qty. Rate Total' in line:\n",
    "            in_menu_section = True\n",
    "            continue\n",
    "            \n",
    "        # Stop capturing when we hit totals section\n",
    "        if 'Total Qty:' in line or 'Sub Total:' in line:\n",
    "            in_menu_section = False\n",
    "            continue\n",
    "        \n",
    "        if in_menu_section:\n",
    "            # Try to extract menu items\n",
    "            # Pattern: [Item name] [quantity] [rate] [total]\n",
    "            parts = line.split()\n",
    "            if len(parts) >= 4:  # Need at least item name, qty, rate, total\n",
    "                try:\n",
    "                    # Last number is the total price\n",
    "                    total = float(parts[-1])\n",
    "                    # Remove the last 3 numbers (qty, rate, total) to get item name\n",
    "                    item_name = ' '.join(parts[:-3])\n",
    "                    # Clean up any special characters\n",
    "                    item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                    \n",
    "                    if item_name and len(item_name) > 2:\n",
    "                        food_items.append(item_name.title())\n",
    "                        prices.append(total)\n",
    "                except ValueError:\n",
    "                    # Handle cases like \"HYDERABADI MURG) BIRYANI\"\n",
    "                    if 'BIRYANI' in line:\n",
    "                        # Special handling for biryani line\n",
    "                        biryani_parts = [p for p in parts if p.replace('.', '').isdigit()]\n",
    "                        if biryani_parts:\n",
    "                            total = float(biryani_parts[-1])\n",
    "                            item_name = ' '.join([p for p in parts if not p.replace('.', '').isdigit()])\n",
    "                            item_name = re.sub(r'[^\\s]', '', item_name)\n",
    "                            food_items.append(item_name.title())\n",
    "                            prices.append(total)\n",
    "    \n",
    "    return {\n",
    "        'food_items': food_items,\n",
    "        'prices': prices\n",
    "    }\n",
    "\n",
    "def generate_summary(processed_text):\n",
    "    \"\"\"\n",
    "    Generate a descriptive summary of the food items and bill\n",
    "    \"\"\"\n",
    "    items = processed_text['food_items']\n",
    "    prices = processed_text['prices']\n",
    "    \n",
    "    if not items:\n",
    "        return \"No food items found in the bill.\"\n",
    "    \n",
    "    summary = \"The bill includes \"\n",
    "    if len(items) == 1:\n",
    "        summary += f\"{items[0]}\"\n",
    "    elif len(items) == 2:\n",
    "        summary += f\"{items[0]} and {items[1]}\"\n",
    "    else:\n",
    "        summary += \", \".join(items[:-1]) + f\", and {items[-1]}\"\n",
    "    \n",
    "    if prices:\n",
    "        total = sum(prices)\n",
    "        summary += f\". The total amount is Rs. {total:.2f}\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_food_entities_with_spacy(text):\n",
    "    \"\"\"\n",
    "    Use spaCy NER to identify food items and related entities\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom food entity patterns\n",
    "        food_patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "        ]\n",
    "        \n",
    "        # Add entity ruler to the pipeline\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        ruler.add_patterns(food_patterns)\n",
    "        \n",
    "        # Process the text\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Return the identified food entities\n",
    "        food_entities = [ent.text for ent in doc.ents if ent.label_ == \"FOOD\"]\n",
    "        \n",
    "        return food_entities\n",
    "    \n",
    "    except OSError:\n",
    "        print(\"Error: The spaCy English model is not installed.\")\n",
    "        print(\"Please run: !python -m spacy download en_core_web_sm\")\n",
    "        return []\n",
    "\n",
    "# Process the bill text to extract food items\n",
    "food_items = extract_food_entities_with_spacy(extracted_text)\n",
    "print(\"Food items identified using spaCy NER:\")\n",
    "for item in food_items:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "def apply_rule_based_classification(text):\n",
    "    \"\"\"\n",
    "    Apply rule-based token classification with regex patterns to identify domain-specific entities\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Dictionary to store all recognized entities\n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # Define patterns for different entity types\n",
    "    patterns = {\n",
    "        'FOOD_ITEM': [\n",
    "            r'(?:Tandoori|Lasooni|HYDERABADI|BIRYANI|Dal|Tadka|Roti|chicken|spicy)\\s*[\\w\\s]*',  # Food items\n",
    "            r'[A-Z][a-z]+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*'  # Capitalized multi-word items\n",
    "        ],\n",
    "        'PRICE': [\n",
    "            r'(?:Rs\\.?|₹)?\\s*\\d+(?:[,.]\\d{1,2})?',  # Indian currency format\n",
    "            r'\\d+\\.\\d{2}'  # Decimal price format\n",
    "        ],\n",
    "        'QUANTITY': [\n",
    "            r'\\b\\d+\\b(?!\\.\\d+)',  # Simple numbers like 1, 2, 3\n",
    "            r'\\d+\\s*x',  # Format: 2x\n",
    "            r'x\\s*\\d+',  # Format: x2\n",
    "            r'\\d+\\s+(?:pcs|items?|pieces)'  # Format: 2 items, 3 pieces\n",
    "        ],\n",
    "        'DATE': [\n",
    "            r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "            r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}'  # 20 May 18\n",
    "        ],\n",
    "        'TIME': [\n",
    "            r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?'  # 12:55, 12:55 PM\n",
    "        ],\n",
    "        'ORDER_NUMBER': [\n",
    "            r'(?:Order|Invoice|Bill)(?:\\s+#|:|\\s+Number:?)\\s*[A-Za-z0-9-]+',  # Invoice Number: IN001001259\n",
    "            r'#\\s*[A-Za-z0-9-]+'  # #12345\n",
    "        ],\n",
    "        'CONTACT': [\n",
    "            r'(?:\\+\\d{1,3}\\s*)?(?:\\(\\d{3,4}\\)\\s*|\\d{3,4}[-\\s])\\d{3,4}[-\\s]\\d{4}',  # Phone number formats\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'  # Email\n",
    "        ],\n",
    "        'ADDRESS': [\n",
    "            r'\\d+/\\d+\\s+\\w+[-\\s]\\s*\\d+',  # 11/2 Sector- 37\n",
    "            r'[A-Z][a-z]+[-\\s]\\s*\\d+'  # Faridabad- 121003\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Apply each pattern and extract matches\n",
    "    for entity_type, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                # Get the matched text and its position\n",
    "                start, end = match.span()\n",
    "                matched_text = match.group()\n",
    "                \n",
    "                # Some post-processing and validation\n",
    "                if entity_type == 'FOOD_ITEM' and len(matched_text) < 3:\n",
    "                    continue  # Skip very short food items\n",
    "                \n",
    "                if entity_type == 'PRICE' and not any(c.isdigit() for c in matched_text):\n",
    "                    continue  # Ensure prices contain digits\n",
    "                \n",
    "                # Store the entity with its position\n",
    "                entities[entity_type].append({\n",
    "                    'text': matched_text,\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def enhance_ner_with_rules(text):\n",
    "    \"\"\"\n",
    "    Combine spaCy NER with rule-based classification to get the best of both approaches\n",
    "    \"\"\"\n",
    "    # Get standard NER results\n",
    "    try:\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Add custom entity patterns\n",
    "        ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "        \n",
    "        # Define patterns for food items and other domain-specific entities\n",
    "        patterns = [\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tandoori\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"biryani\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"roti\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dal\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tadka\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"spicy\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lasooni\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"hyderabadi\"}]},\n",
    "            {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"murg\"}]},\n",
    "            {\"label\": \"QUANTITY\", \"pattern\": [{\"IS_DIGIT\": True}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"ddd.dd\"}]},\n",
    "            {\"label\": \"PRICE\", \"pattern\": [{\"SHAPE\": \"d,ddd.dd\"}]},\n",
    "        ]\n",
    "        \n",
    "        ruler.add_patterns(patterns)\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        spacy_entities = {\n",
    "            \"FOOD\": [],\n",
    "            \"CARDINAL\": [],\n",
    "            \"MONEY\": [],\n",
    "            \"DATE\": [],\n",
    "            \"TIME\": [],\n",
    "            \"ORG\": [],\n",
    "            \"PERSON\": [],\n",
    "        }\n",
    "        \n",
    "        # Extract entities recognized by spaCy\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ in spacy_entities:\n",
    "                spacy_entities[ent.label_].append({\n",
    "                    'text': ent.text,\n",
    "                    'start': ent.start_char,\n",
    "                    'end': ent.end_char\n",
    "                })\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in spaCy processing: {e}\")\n",
    "        spacy_entities = {}\n",
    "    \n",
    "    # Get rule-based entities\n",
    "    rule_entities = apply_rule_based_classification(text)\n",
    "    \n",
    "    # Merge the results with priority to rule-based entities\n",
    "    combined_entities = {}\n",
    "    \n",
    "    # First add rule-based entities\n",
    "    for entity_type, entities in rule_entities.items():\n",
    "        combined_entities[entity_type] = entities\n",
    "    \n",
    "    # Then add spaCy entities that don't overlap with rule-based ones\n",
    "    for entity_type, entities in spacy_entities.items():\n",
    "        if entity_type not in combined_entities:\n",
    "            combined_entities[entity_type] = []\n",
    "        \n",
    "        for entity in entities:\n",
    "            # Check if this entity overlaps with any existing entity\n",
    "            overlaps = False\n",
    "            for existing_type in combined_entities:\n",
    "                for existing_entity in combined_entities[existing_type]:\n",
    "                    # Check for overlap in character spans\n",
    "                    if (entity['start'] < existing_entity['end'] and \n",
    "                        entity['end'] > existing_entity['start']):\n",
    "                        overlaps = True\n",
    "                        break\n",
    "                if overlaps:\n",
    "                    break\n",
    "            \n",
    "            # Add if no overlap\n",
    "            if not overlaps:\n",
    "                combined_entities[entity_type].append(entity)\n",
    "    \n",
    "    return combined_entities\n",
    "\n",
    "print(\"\\nApplying rule-based token classification to receipt text:\")\n",
    "rule_based_entities = apply_rule_based_classification(extracted_text)\n",
    "for entity_type, entities in rule_based_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")\n",
    "\n",
    "print(\"\\nCombining spaCy NER with rule-based classification:\")\n",
    "combined_entities = enhance_ner_with_rules(extracted_text)\n",
    "for entity_type, entities in combined_entities.items():\n",
    "    if entities:\n",
    "        print(f\"\\n{entity_type}:\")\n",
    "        for entity in entities:\n",
    "            print(f\"  • {entity['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Install required libraries for BERT model implementation\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm\n",
    "!pip install pandas\n",
    "!pip install matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Import libraries for BERT implementation\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel, BertConfig, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate cropping and OCR into a single function\n",
    "def process_image_to_text(image_path):\n",
    "    \"\"\"\n",
    "    Process an image: read, crop, and extract text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the image\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        # Extract text using pytesseract\n",
    "        extracted_text = pytesseract.image_to_string(image)\n",
    "        \n",
    "        # Clean the extracted text\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        \n",
    "        return {\n",
    "            \"original_text\": extracted_text,\n",
    "            \"cleaned_text\": cleaned_text,\n",
    "            \"success\": True\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text for BERT processing\n",
    "    \"\"\"\n",
    "    # Remove special characters and normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'[^\\w\\s.,;:!?$%&()-]', '', text)  # Keep only alphanumeric and basic punctuation\n",
    "    \n",
    "    # Remove redundant line breaks\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "# Test the function with your image\n",
    "result = process_image_to_text(image_path)\n",
    "if result[\"success\"]:\n",
    "    print(\"Original Extracted Text:\")\n",
    "    print(result[\"original_text\"][:500], \"...\\n\")\n",
    "    print(\"Cleaned Text for BERT:\")\n",
    "    print(result[\"cleaned_text\"][:500], \"...\")\n",
    "else:\n",
    "    print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class for BERT\n",
    "class ReceiptDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# For demonstration, create a dummy dataset or load real data if available\n",
    "def prepare_sample_dataset():\n",
    "    \"\"\"\n",
    "    Create a sample dataset for demonstration. \n",
    "    In a real scenario, you would load actual receipt data.\n",
    "    \"\"\"\n",
    "    # Simulate different receipt types (e.g., restaurant, grocery, etc.)\n",
    "    receipt_types = [\n",
    "        \"restaurant receipt\", \"grocery receipt\", \"cafe bill\", \n",
    "        \"hotel invoice\", \"retail purchase\"\n",
    "    ]\n",
    "    \n",
    "    # Generate data for demonstration\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    # Let's create 100 sample records\n",
    "    for i in range(100):\n",
    "        # For demonstrating a multi-class problem (classify receipt type)\n",
    "        label = i % len(receipt_types)\n",
    "        \n",
    "        # Generate synthetic text with characteristics of that receipt type\n",
    "        if label == 0:  # restaurant\n",
    "            text = f\"Restaurant name: Sample Restaurant {i}\\nServer: John\\nTable: 12\\nItems: Burger $10.99, Fries $4.99, Drink $2.99\\nSubtotal: $18.97\\nTax: $1.52\\nTotal: $20.49\\nTip: _______\\nThank you!\"\n",
    "        elif label == 1:  # grocery\n",
    "            text = f\"Grocery Market {i}\\n123 Main St\\nItems:\\nMilk $3.99\\nBread $2.49\\nEggs $4.29\\nApples $5.99\\nTotal: $16.76\\nTax: $1.34\\nTotal with tax: $18.10\\nPaid with card: VISA **** 1234\"\n",
    "        elif label == 2:  # cafe\n",
    "            text = f\"Cafe Coffee {i}\\nOrder #45678\\nItems:\\nLatte $4.50\\nCroissant $3.25\\nSubtotal: $7.75\\nTax: $0.62\\nTotal: $8.37\\nThank you for visiting!\"\n",
    "        elif label == 3:  # hotel\n",
    "            text = f\"Hotel Paradise {i}\\nGuest: Alex Smith\\nRoom: 302\\nCheck-in: 01/15/2023\\nCheck-out: 01/18/2023\\nRoom charge: $399.99\\nResort fee: $45.00\\nTax: $44.50\\nTotal: $489.49\\nThank you for staying with us!\"\n",
    "        else:  # retail\n",
    "            text = f\"Retail Store {i}\\n555 Shopping Ave\\nItems:\\nT-shirt $19.99\\nJeans $49.99\\nSocks $8.99\\nSubtotal: $78.97\\nDiscount: -$10.00\\nTax: $5.52\\nTotal: $74.49\\nPaid with: CREDIT\"\n",
    "        \n",
    "        texts.append(clean_text(text))\n",
    "        labels.append(label)\n",
    "    \n",
    "    return texts, labels, receipt_types\n",
    "\n",
    "# Get or create dataset\n",
    "texts, labels, class_names = prepare_sample_dataset()\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")\n",
    "print(f\"Class labels: {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BERT Model for text classification\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained_model=\"bert-base-uncased\"):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(pretrained_model)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # Get BERT outputs\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # Use the [CLS] token representation for classification\n",
    "        pooled_output = outputs.pooler_output\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "num_classes = len(class_names)\n",
    "model = BERTClassifier(num_classes=num_classes)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ReceiptDataset(X_train, y_train, tokenizer, max_len=256)\n",
    "test_dataset = ReceiptDataset(X_test, y_test, tokenizer, max_len=256)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Function to create a progress bar that works in any environment\n",
    "def get_progress_bar(iterable, desc=None):\n",
    "    # Try to use tqdm if available\n",
    "    try:\n",
    "        # Try regular tqdm first (avoid notebook issues)\n",
    "        from tqdm import tqdm\n",
    "        return tqdm(iterable, desc=desc)\n",
    "    except (ImportError, Exception) as e:\n",
    "        # Fallback to a simple iterator if tqdm is not available\n",
    "        print(f\"{desc}: {len(iterable)} batches\")\n",
    "        return iterable\n",
    "\n",
    "# Training function with epoch-by-epoch visualization\n",
    "def train_model(model, train_dataloader, test_dataloader, device, class_names, epochs=100):\n",
    "    # Initialize optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Define loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # To store training history\n",
    "    training_stats = []\n",
    "    epoch_train_losses = []\n",
    "    epoch_val_losses = []\n",
    "    epoch_train_accs = []\n",
    "    epoch_val_accs = []\n",
    "    \n",
    "    # Set up plots for live updating\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax2 = plt.subplot(122)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n{'='*50}\\nEpoch {epoch+1}/{epochs}\\n{'='*50}\")\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_train_correct = 0\n",
    "        total_train_samples = 0\n",
    "        \n",
    "        # Progress bar for training - using our robust implementation\n",
    "        train_progress_bar = get_progress_bar(train_dataloader, desc=\"Training\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Move batch to device\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Clear gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            \n",
    "            # Handle different output types (tensor vs object with logits)\n",
    "            logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Track stats\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # Track accuracy\n",
    "            _, predicted = torch.max(logits, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total_train_correct += correct\n",
    "            total_train_samples += labels.size(0)\n",
    "            \n",
    "            # Regular print for non-tqdm progress\n",
    "            if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Training Batch {batch_idx}/{len(train_dataloader)}: loss={loss.item():.4f}, acc={correct/labels.size(0):.4f}\")\n",
    "        \n",
    "        # Calculate average training loss and accuracy for this epoch\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_accuracy = total_train_correct / total_train_samples\n",
    "        \n",
    "        print(f\"Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        total_eval_loss = 0\n",
    "        total_eval_correct = 0\n",
    "        total_eval_samples = 0\n",
    "        all_predictions = []\n",
    "        all_true_labels = []\n",
    "        \n",
    "        # No gradients needed for evaluation\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # Move batch to device\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                token_type_ids = batch['token_type_ids'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "                \n",
    "                # Handle different output types (tensor vs object with logits)\n",
    "                logits = outputs.logits if hasattr(outputs, 'logits') else outputs\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = loss_fn(logits, labels)\n",
    "                total_eval_loss += loss.item()\n",
    "                \n",
    "                # Track accuracy\n",
    "                _, predicted = torch.max(logits, 1)\n",
    "                correct = (predicted == labels).sum().item()\n",
    "                total_eval_correct += correct\n",
    "                total_eval_samples += labels.size(0)\n",
    "                \n",
    "                # Collect predictions for metrics\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "                all_true_labels.extend(labels.cpu().numpy())\n",
    "                \n",
    "                # Regular print for progress\n",
    "                if batch_idx % 10 == 0:  # Print every 10 batches\n",
    "                    print(f\"Eval Batch {batch_idx}/{len(test_dataloader)}: loss={loss.item():.4f}, acc={correct/labels.size(0):.4f}\")\n",
    "        \n",
    "        # Calculate average validation loss and accuracy\n",
    "        avg_val_loss = total_eval_loss / len(test_dataloader)\n",
    "        val_accuracy = total_eval_correct / total_eval_samples\n",
    "        \n",
    "        print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Store epoch statistics\n",
    "        epoch_train_losses.append(avg_train_loss)\n",
    "        epoch_val_losses.append(avg_val_loss)\n",
    "        epoch_train_accs.append(train_accuracy)\n",
    "        epoch_val_accs.append(val_accuracy)\n",
    "        \n",
    "        # Update plots\n",
    "        epochs_range = list(range(1, epoch + 2))\n",
    "        \n",
    "        # Compute classification report for this epoch\n",
    "        report = classification_report(all_true_labels, all_predictions, \n",
    "                                      target_names=class_names, zero_division=1)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(report)\n",
    "        \n",
    "        # Save stats\n",
    "        training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'train_acc': train_accuracy,\n",
    "            'val_acc': val_accuracy,\n",
    "            'classification_report': report\n",
    "        })\n",
    "    \n",
    "    # Save model\n",
    "    model_path = os.path.join(os.getcwd(), 'bert_receipt_classifier.pt')\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return training_stats\n",
    "\n",
    "training_stats = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    device=device,\n",
    "    class_names=class_names,\n",
    "    epochs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Function to test model on new receipts\n",
    "def classify_receipt_text(text, model, tokenizer, class_names):\n",
    "    \"\"\"\n",
    "    Classify a receipt text using the trained BERT model\n",
    "    \"\"\"\n",
    "    # Clean the text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Prepare input tensors\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        cleaned_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=256,\n",
    "        return_token_type_ids=True,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move tensors to device\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    token_type_ids = encoding['token_type_ids'].to(device)\n",
    "    \n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "    \n",
    "    # Get predicted class and confidence\n",
    "    predicted_class = class_names[predicted.item()]\n",
    "    confidence = probabilities[0][predicted.item()].item()\n",
    "    \n",
    "    # Return all class probabilities for visualization\n",
    "    all_probs = {class_name: prob.item() for class_name, prob in zip(class_names, probabilities[0])}\n",
    "    \n",
    "    return {\n",
    "        'class': predicted_class,\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': all_probs\n",
    "    }\n",
    "\n",
    "# Test on new receipts from the test dataset\n",
    "for i in range(5):  # Test 5 random samples\n",
    "    idx = random.randint(0, len(X_test) - 1)\n",
    "    text = X_test[idx]\n",
    "    true_label = class_names[y_test[idx]]\n",
    "    \n",
    "    # Classify the receipt\n",
    "    result = classify_receipt_text(text, model, tokenizer, class_names)\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {text[:150]}...\")\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Predicted label: {result['class']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.4f}\")\n",
    "    \n",
    "    # Visualize all probabilities\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(result['all_probabilities'].keys(), result['all_probabilities'].values())\n",
    "    plt.title(f\"Class Probabilities for Sample {i+1}\")\n",
    "    plt.xlabel(\"Class\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytesseract\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# 🔹 Step 1: OCR Setup\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "# 🔹 Step 2: Extract Text from Cropped Receipts\n",
    "def extract_text_from_images(folder_path):\n",
    "    data = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(('.png', '.jpg', '.jpeg')):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            text = pytesseract.image_to_string(img)  # OCR extraction\n",
    "            data.append({\"filename\": file, \"text\": text.strip()})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "folder_path = r\"C:\\Users\\nanda\\OneDrive\\Desktop\\NLP-Mini-Project\\Cropped_Receipts\"\n",
    "df = extract_text_from_images(folder_path)\n",
    "\n",
    "# 🔹 Step 3: Automated Labeling Rules\n",
    "def classify_text(text):\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Rule 1: Amount Detection ($XX.XX or XX.XX format)\n",
    "    if re.search(r\"\\$\\d+\\.\\d{2}|\\b\\d+\\.\\d{2}\\b\", text):\n",
    "        return \"total_amount\"\n",
    "\n",
    "    # Rule 2: Date Detection (MM/DD/YYYY or DD/MM/YYYY)\n",
    "    if re.search(r\"\\b\\d{1,2}/\\d{1,2}/\\d{4}\\b\", text):\n",
    "        return \"date\"\n",
    "\n",
    "    # Rule 3: Restaurant Name Detection (Known Chains)\n",
    "    restaurant_keywords = [\"McDonald's\", \"Subway\", \"Starbucks\", \"KFC\", \"Pizza Hut\", \"Domino's\", \"Burger King\"]\n",
    "    if any(kw.lower() in text.lower() for kw in restaurant_keywords):\n",
    "        return \"restaurant_name\"\n",
    "\n",
    "    # Rule 4: Item List Detection (Comma-separated items)\n",
    "    if \",\" in text and len(text.split()) > 2:\n",
    "        return \"items\"\n",
    "    \n",
    "    return \"other\"\n",
    "\n",
    "df[\"category\"] = df[\"text\"].apply(classify_text)\n",
    "\n",
    "# 🔹 Step 4: Save Auto-Labeled Data for BERT Training\n",
    "df = df[df[\"category\"] != \"other\"]  # Remove unclassified\n",
    "df.to_csv(\"auto_labeled_receipt_data.csv\", index=False)\n",
    "\n",
    "print(\"✅ Auto-labeling complete! Data saved as auto_labeled_receipt_data.csv\")\n",
    "\n",
    "# 🔹 Step 5: Load Tokenizer & Prepare Data\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "category_mapping = {cat: idx for idx, cat in enumerate(df[\"category\"].unique())}\n",
    "df[\"label\"] = df[\"category\"].map(category_mapping)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"label\"], test_size=0.2, random_state=42)\n",
    "\n",
    "class ReceiptDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text, add_special_tokens=True, max_length=self.max_len,\n",
    "            return_token_type_ids=False, padding=\"max_length\",\n",
    "            truncation=True, return_attention_mask=True, return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_dataset = ReceiptDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_len=64)\n",
    "test_dataset = ReceiptDataset(X_test.tolist(), y_test.tolist(), tokenizer, max_len=64)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "test_dataloader = DataLoader(test_dataset, sampler=RandomSampler(test_dataset), batch_size=8)\n",
    "\n",
    "# 🔹 Step 6: Define BERT Model\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERTClassifier(num_classes=len(category_mapping)).to(device)\n",
    "\n",
    "# 🔹 Step 7: Train the Model\n",
    "def train_model(model, train_dataloader, epochs=100):\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss, total_correct, total_samples = 0, 0, 0\n",
    "\n",
    "        for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\"):\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_train_loss/len(train_dataloader):.4f}, Accuracy = {total_correct/total_samples:.4f}\")\n",
    "\n",
    "train_model(model, train_dataloader, epochs=100)\n",
    "\n",
    "# 🔹 Step 8: Evaluate the Model\n",
    "def evaluate_model(model, test_dataloader):\n",
    "    model.eval()\n",
    "    total_correct, total_samples = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids, attention_mask, labels = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device), batch[\"labels\"].to(device)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = total_correct / total_samples\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    # Fix: Get only the present labels in predictions\n",
    "    present_labels = sorted(set(all_labels))  \n",
    "    target_names = [k for k, v in category_mapping.items() if v in present_labels]\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, labels=present_labels, target_names=target_names))\n",
    "\n",
    "evaluate_model(model, test_dataloader)\n",
    "\n",
    "# 🔹 Step 9: Predict on New Receipt Text\n",
    "def predict_category(text):\n",
    "    model.eval()\n",
    "    encoding = tokenizer.encode_plus(text, return_tensors=\"pt\", max_length=64, padding=\"max_length\", truncation=True)\n",
    "    input_ids, attention_mask = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    category = [k for k, v in category_mapping.items() if v == pred][0]\n",
    "    return category\n",
    "\n",
    "# Example Prediction\n",
    "sample_text = \"$14.89\"\n",
    "print(f\"Predicted Category: {predict_category(sample_text)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
