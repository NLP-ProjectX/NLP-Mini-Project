{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs('Cropped_Receipts', exist_ok=True)\n",
    "\n",
    "# ADD YOUR FOLDER PATH\n",
    "input_folder = 'dataset'\n",
    "output_folder = 'Cropped_Receipts'\n",
    "\n",
    "# Gamma correction\n",
    "invGamma = 1.0 / 0.3\n",
    "table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "def biggestRectangle(contours):\n",
    "    biggest = None\n",
    "    max_area = 0\n",
    "    indexReturn = -1\n",
    "    for index in range(len(contours)):\n",
    "        i = contours[index]\n",
    "        area = cv2.contourArea(i)\n",
    "        if area > 100:\n",
    "            peri = cv2.arcLength(i, True)\n",
    "            approx = cv2.approxPolyDP(i, 0.1 * peri, True)\n",
    "            if area > max_area:  \n",
    "                biggest = approx\n",
    "                max_area = area\n",
    "                indexReturn = index\n",
    "    return indexReturn\n",
    "\n",
    "# Get all image files from input folder\n",
    "image_files = glob.glob(os.path.join(input_folder, '*.jpg'))\n",
    "\n",
    "# Process each image\n",
    "for idx, image_path in enumerate(image_files):\n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply gamma correction\n",
    "    gray = cv2.LUT(gray, table)\n",
    "\n",
    "    # Thresholding\n",
    "    ret, thresh1 = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours\n",
    "    contours, hierarchy = cv2.findContours(thresh1, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the biggest rectangle contour\n",
    "    indexReturn = biggestRectangle(contours)\n",
    "\n",
    "    if indexReturn != -1:\n",
    "        # Get the bounding box for the largest contour\n",
    "        hull = cv2.convexHull(contours[indexReturn])\n",
    "        x, y, w, h = cv2.boundingRect(hull)\n",
    "        cv2.drawContours(img, [hull], 0, (0, 255, 0), 3)\n",
    "        \n",
    "        # Crop the ROI\n",
    "        cropped = img[y:y+h, x:x+w]\n",
    "\n",
    "        # Save cropped image with incremental name\n",
    "        cropped_filename = os.path.join(output_folder, f'cropped_receipt_{idx+1}.png')\n",
    "        cv2.imwrite(cropped_filename, cropped)\n",
    "        print(f\"Saved: {cropped_filename}\")\n",
    "\n",
    "        # # Optional: Display cropped image\n",
    "        # plt.imshow(cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB))\n",
    "        # plt.axis(\"off\")\n",
    "        # plt.title(f\"Cropped Receipt {idx+1}\")\n",
    "        # plt.show()\n",
    "    else:\n",
    "        print(f\"No valid contours found for image: {os.path.basename(image_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "!pip install spacy\n",
    "# Download the spaCy English model - this is crucial\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.util import filter_spans\n",
    "\n",
    "def identify_food_entities(processed_text):\n",
    "    \"\"\"\n",
    "    Use spaCy's NER to identify food items, prices, quantities, and dates in the bill text.\n",
    "    Custom entity rules are added to recognize food items and menu-specific terminology.\n",
    "    \n",
    "    Args:\n",
    "        processed_text: The preprocessed text from the hotel bill\n",
    "        \n",
    "    Returns:\n",
    "        doc: spaCy Doc object with entities identified\n",
    "        extracted_entities: Dictionary containing the extracted entities by category\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        # If model not found, provide helpful error\n",
    "        print(\"Error: The spaCy English model is not installed.\")\n",
    "        print(\"Please run: !python -m spacy download en_core_web_sm\")\n",
    "        return None, {}\n",
    "    \n",
    "    # Add custom food entity patterns to the pipeline\n",
    "    # This helps recognize food items that aren't in the default NER model\n",
    "    food_patterns = [\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"coffee\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lunch\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dinner\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"breakfast\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"salad\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"sandwich\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"burger\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"pizza\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"pasta\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"steak\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"fish\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"coke\"}, {\"LOWER\": \"cola\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"water\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"juice\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tea\"}]},\n",
    "    ]\n",
    "    \n",
    "    # Create the matcher and add it to the pipeline\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    ruler.add_patterns(food_patterns)\n",
    "    \n",
    "    # Process the text\n",
    "    doc = nlp(processed_text)\n",
    "    \n",
    "    # Extract entities by category\n",
    "    extracted_entities = {\n",
    "        \"FOOD\": [],\n",
    "        \"MONEY\": [],\n",
    "        \"QUANTITY\": [],\n",
    "        \"DATE\": [],\n",
    "        \"PERSON\": [],  # For server names\n",
    "        \"ORG\": [],     # For restaurant names\n",
    "    }\n",
    "    \n",
    "    # Custom recognition for prices and quantities using regex\n",
    "    import re\n",
    "    \n",
    "    # Find prices (e.g., $12.99, 12.99, 12)\n",
    "    price_pattern = r'\\$?\\d+\\.?\\d*'\n",
    "    for match in re.finditer(price_pattern, processed_text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end, label=\"MONEY\")\n",
    "        if span is not None:\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Find quantities (e.g., 2x, x2, 2 items)\n",
    "    quantity_pattern = r'\\d+\\s*x|\\bx\\s*\\d+|\\b\\d+\\s+(?:items?|pcs)\\b'\n",
    "    for match in re.finditer(quantity_pattern, processed_text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start, end, label=\"QUANTITY\")\n",
    "        if span is not None:\n",
    "            doc.ents = list(doc.ents) + [span]\n",
    "    \n",
    "    # Remove overlapping entities\n",
    "    doc.ents = filter_spans(list(doc.ents))\n",
    "    \n",
    "    # Categorize entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in extracted_entities:\n",
    "            extracted_entities[ent.label_].append((ent.text, ent.start_char, ent.end_char))\n",
    "    \n",
    "    return doc, extracted_entities\n",
    "\n",
    "def display_entities(doc, extracted_entities):\n",
    "    \"\"\"\n",
    "    Display the extracted entities in a readable format\n",
    "    \n",
    "    Args:\n",
    "        doc: spaCy Doc object with entities identified\n",
    "        extracted_entities: Dictionary containing the extracted entities by category\n",
    "    \"\"\"\n",
    "    print(\"Extracted Entities:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for category, entities in extracted_entities.items():\n",
    "        if entities:\n",
    "            print(f\"\\n{category}:\")\n",
    "            for entity in entities:\n",
    "                print(f\"  â€¢ {entity[0]}\")\n",
    "    \n",
    "    print(\"\\nEntity visualization:\")\n",
    "    print(spacy.displacy.render(doc, style=\"ent\", jupyter=False))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example processed text from a hotel bill\n",
    "    processed_text = \"\"\"\n",
    "    GREEN FIELD\n",
    "    5305 E PACIFIC COAST HWY\n",
    "    Long Beach, CA 90004\n",
    "    (562) 597-0906\n",
    "    \n",
    "    Server: Francis\n",
    "    Order #: 69923\n",
    "    Table: B11\n",
    "    Guests: 2\n",
    "    \n",
    "    1 Coffee              3.00\n",
    "    2 Lunch              45.90\n",
    "    1 Coke                3.00\n",
    "    \n",
    "    SUB TOTAL:           51.90\n",
    "    Tax:                  4.60\n",
    "    \n",
    "    TOTAL:               $56.50\n",
    "    \n",
    "    5/26/2016 12:53:10 PM\n",
    "    \n",
    "    THANK YOU!\n",
    "    \"\"\"\n",
    "    \n",
    "    doc, entities = identify_food_entities(processed_text)\n",
    "    display_entities(doc, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rule_based_classification(text):\n",
    "    \"\"\"\n",
    "    Apply rule-based token classification with regex patterns to identify domain-specific entities\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Dictionary to store all recognized entities\n",
    "    entities = defaultdict(list)\n",
    "    \n",
    "    # Define patterns for different entity types\n",
    "    patterns = {\n",
    "        'FOOD_ITEM': [\n",
    "            r'(?:Coffee|Lunch|Dinner|Breakfast|Salad|Sandwich|Burger|Pizza|Pasta|Steak|Chicken|Fish|Coke|Water|Juice|Tea)\\s*[\\w\\s]*',\n",
    "            r'[A-Z][a-z]+\\s+[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*'  # Capitalized multi-word items\n",
    "        ],\n",
    "        'PRICE': [\n",
    "            r'\\$\\s*\\d+(?:[,.]\\d{1,2})?',  # US currency format\n",
    "            r'\\d+\\.\\d{2}'  # Decimal price format\n",
    "        ],\n",
    "        'QUANTITY': [\n",
    "            r'\\b\\d+\\b(?!\\.\\d+)',  # Simple numbers like 1, 2, 3\n",
    "            r'\\d+\\s*x',  # Format: 2x\n",
    "            r'x\\s*\\d+',  # Format: x2\n",
    "            r'\\d+\\s+(?:pcs|items?|pieces)'  # Format: 2 items, 3 pieces\n",
    "        ],\n",
    "        'DATE': [\n",
    "            r'\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}',  # DD/MM/YYYY or MM/DD/YYYY\n",
    "            r'\\d{1,2}\\s+(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\s+\\d{2,4}'  # 20 May 18\n",
    "        ],\n",
    "        'TIME': [\n",
    "            r'\\d{1,2}:\\d{2}(?::\\d{2})?\\s*(?:AM|PM|am|pm)?'  # 12:55, 12:55 PM\n",
    "        ],\n",
    "        'ORDER_NUMBER': [\n",
    "            r'(?:Order|Invoice|Bill)(?:\\s+#|:|\\s+Number:?)\\s*[A-Za-z0-9-]+',\n",
    "            r'#\\s*[A-Za-z0-9-]+'  # #12345\n",
    "        ],\n",
    "        'CONTACT': [\n",
    "            r'(?:\\+\\d{1,3}\\s*)?(?:\\(\\d{3,4}\\)\\s*|\\d{3,4}[-\\s])\\d{3,4}[-\\s]\\d{4}',  # Phone number formats\n",
    "            r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'  # Email\n",
    "        ],\n",
    "        'ADDRESS': [\n",
    "            r'\\d+\\s+[A-Za-z\\s]+(?:St|Street|Ave|Avenue|Blvd|Boulevard|Rd|Road|Hwy|Highway)',\n",
    "            r'[A-Za-z]+,\\s*[A-Z]{2}\\s+\\d{5}'  # City, State ZIP\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Apply each pattern and extract matches\n",
    "    for entity_type, pattern_list in patterns.items():\n",
    "        for pattern in pattern_list:\n",
    "            matches = re.finditer(pattern, text)\n",
    "            for match in matches:\n",
    "                # Get the matched text and its position\n",
    "                start, end = match.span()\n",
    "                matched_text = match.group()\n",
    "                \n",
    "                # Store the entity with its position\n",
    "                entities[entity_type].append({\n",
    "                    'text': matched_text,\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def enhance_spacy_ner(text, extracted_entities=None):\n",
    "    \"\"\"\n",
    "    Enhance spaCy NER with rule-based token classification\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        # Process text with spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Get entities recognized by spaCy\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            entities.append({\n",
    "                'text': ent.text,\n",
    "                'start': ent.start_char,\n",
    "                'end': ent.end_char,\n",
    "                'label': ent.label_\n",
    "            })\n",
    "        \n",
    "        # If we have rule-based entities, add them\n",
    "        if extracted_entities:\n",
    "            for entity_type, entity_list in extracted_entities.items():\n",
    "                for entity in entity_list:\n",
    "                    # Check if this entity overlaps with any spaCy entity\n",
    "                    overlaps = False\n",
    "                    for spacy_entity in entities:\n",
    "                        if (entity['start'] < spacy_entity['end'] and \n",
    "                            entity['end'] > spacy_entity['start']):\n",
    "                            overlaps = True\n",
    "                            break\n",
    "                    \n",
    "                    # Add if no overlap\n",
    "                    if not overlaps:\n",
    "                        entities.append({\n",
    "                            'text': entity['text'],\n",
    "                            'start': entity['start'],\n",
    "                            'end': entity['end'],\n",
    "                            'label': entity_type\n",
    "                        })\n",
    "        \n",
    "        return entities\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error enhancing NER: {e}\")\n",
    "        return []\n",
    "\n",
    "# Modify the identify_food_entities function to incorporate rule-based classification\n",
    "def identify_food_entities(processed_text):\n",
    "    \"\"\"\n",
    "    Use spaCy's NER combined with rule-based classification to identify entities in the bill text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load spaCy model\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except OSError:\n",
    "        print(\"Error: The spaCy English model is not installed.\")\n",
    "        print(\"Please run: !python -m spacy download en_core_web_sm\")\n",
    "        return None, {}\n",
    "    \n",
    "    # First apply rule-based classification\n",
    "    rule_based_entities = apply_rule_based_classification(processed_text)\n",
    "    \n",
    "    # Add custom food entity patterns to the pipeline\n",
    "    food_patterns = [\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"coffee\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"lunch\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"dinner\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"breakfast\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"salad\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"sandwich\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"burger\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"pizza\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"pasta\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"steak\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"chicken\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"fish\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"coke\"}, {\"LOWER\": \"cola\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"water\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"juice\"}]},\n",
    "        {\"label\": \"FOOD\", \"pattern\": [{\"LOWER\": \"tea\"}]},\n",
    "    ]\n",
    "    \n",
    "    # Create the matcher and add it to the pipeline\n",
    "    ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "    ruler.add_patterns(food_patterns)\n",
    "    \n",
    "    # Process the text\n",
    "    doc = nlp(processed_text)\n",
    "    \n",
    "    # Extract entities by category\n",
    "    extracted_entities = {\n",
    "        \"FOOD\": [],\n",
    "        \"MONEY\": [],\n",
    "        \"QUANTITY\": [],\n",
    "        \"DATE\": [],\n",
    "        \"PERSON\": [],  # For server names\n",
    "        \"ORG\": [],     # For restaurant names\n",
    "    }\n",
    "    \n",
    "    # Merge rule-based entities with spaCy entities\n",
    "    for entity_type in rule_based_entities:\n",
    "        mapped_type = entity_type\n",
    "        if entity_type == 'FOOD_ITEM':\n",
    "            mapped_type = 'FOOD'\n",
    "        elif entity_type == 'PRICE':\n",
    "            mapped_type = 'MONEY'\n",
    "        \n",
    "        if mapped_type in extracted_entities:\n",
    "            for entity in rule_based_entities[entity_type]:\n",
    "                extracted_entities[mapped_type].append((entity['text'], entity['start'], entity['end']))\n",
    "    \n",
    "    # Add spaCy entities that don't overlap with rule-based ones\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in extracted_entities:\n",
    "            # Check if this entity overlaps with any already extracted entity\n",
    "            overlaps = False\n",
    "            for entity_type in extracted_entities:\n",
    "                for entity in extracted_entities[entity_type]:\n",
    "                    if (ent.start_char < entity[2] and ent.end_char > entity[1]):\n",
    "                        overlaps = True\n",
    "                        break\n",
    "                if overlaps:\n",
    "                    break\n",
    "            \n",
    "            # Add if no overlap\n",
    "            if not overlaps:\n",
    "                extracted_entities[ent.label_].append((ent.text, ent.start_char, ent.end_char))\n",
    "    \n",
    "    return doc, extracted_entities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
